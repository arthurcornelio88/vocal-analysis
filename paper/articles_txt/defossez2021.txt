Hybrid Spectrogram and Waveform Source Separation
Alexandre D√©fossez1
1 Facebook AI Research

arXiv:2111.03600v3 [eess.AS] 30 Aug 2022

License
Authors of papers retain
copyright and release the work
under a Creative Commons
Attribution 4.0 International
License (CC BY 4.0).
In partnership with

Abstract
Source separation models either work on the spectrogram or waveform domain. In this work,
we show how to perform end-to-end hybrid source separation, letting the model decide which
domain is best suited for each source, and even combining both. The proposed hybrid version
of the Demucs architecture (D√©fossez et al., 2019) won the Music Demixing Challenge 2021
organized by Sony. This architecture also comes with additional improvements, such as
compressed residual branches, local attention or singular value regularization. Overall, a 1.4 dB
improvement of the Signal-To-Distortion (SDR) was observed across all sources as measured
on the MusDB HQ dataset (Rafii et al., 2019), an improvement confirmed by human subjective
evaluation, with an overall quality rated at 2.83 out of 5 (2.36 for the non hybrid Demucs),
and absence of contamination at 3.04 (against 2.37 for the non hybrid Demucs and 2.44 for
the second ranking model submitted at the competition).

Introduction
Work on music source separation has recently focused on the task of separating 4 well defined
instruments in a supervised manner: drums, bass, vocals and other accompaniments. Recent
evaluation campaigns (St√∂ter et al., 2018) have focused on this setting, relying on the standard
MusDB benchmark (Rafii et al., 2017). In 2021, Sony organized the Music Demixing Challenge
(MDX) (Mitsufuji et al., 2021), an online competition where separation models are evaluated
on a completely new and hidden test set composed of 27 tracks.
The challenge featured a number of baselines to start from, which could be divided into two
categories: spectrogram or waveform based methods. The former consists in models that
are fed with the input spectrogram, either represented by its amplitude, such as Open-Unmix
(St√∂ter et al., 2019) and its variant CrossNet Open-Unmix (Sawata et al., 2020), or as the
concatenation of its real and imaginary part, a.k.a Complex-As-Channels (CAC) (Choi et al.,
2020), such as LaSAFT (Choi et al., 2021). Similarly, the output can be either a mask on the
input spectrogram, complex modulation of the input spectrogram (Kong et al., 2021), or the
CAC representation.
On the other hand, waveform based models such as Demucs (D√©fossez et al., 2019) are directly
fed with the raw waveform, and output the raw waveform for each of the source. Most of those
methods will perform some kind of learnt time-frequency analysis in its first layers through
convolutions, such as Demucs and Conv-TasNet (Luo and Mesgarani, 2019), although some
will not rely at all on convolutional layers, like Dual-Path RNN (Luo et al., 2020).
Theoretically, there should be no difference between spectrogram and waveform models, in
particular when considering CaC (complex as channels), which is only a linear change of base
for the input and output space. However, this would only hold true in the limit of having
an infinite amount of training data. With a constrained dataset, such as the 100 songs of
MusDB, inductive bias can play an important role. In particular, spectrogram methods varies
by more than their input and output space. For instance, with a notion of frequency, it is
possible to apply convolutions along frequencies, while waveform methods must use layers
that are fully connected with respect to their channels. The final test loss being far from

Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.

1

zero, there will also be artifacts in the separated audio. Different representations will lead to
different artifacts, some being more noticeable for the drums and bass (phase inconsitency for
spectrogram methods will make the attack sounds hollow), while others are more noticeable
for the vocals (vocals separated by Demucs suffer from crunchy static noise)
In this work, we extend the Demucs architecture to perform hybrid waveform/spectrogram
domain source separation. The original U-Net architecture (Ronneberger et al., 2015) is
extended to provide two parallel branches: one in the time (temporal) and one in the frequency
(spectral) domain. We bring other improvements to the architecture, namely compressed
residual branches comprising dilated convolutions (Yu and Koltun, 2016), LSTM (Hochreiter
and Schmidhuber, 1997) and attention (Vaswani et al., 2017) with a focus on local content.
We measure the impact of those changes on the MusDB benchmark and on the MDX hidden
test set, as well as subjective evaluations. Hybrid Demucs ranked 1st at the MDX competition
when trained only on MusDB, with 7.32 dB of SDR, and 2nd with extra training data allowed.

Related work
There exist a number of spectrogram based music source separation architectures. Open-Unmix
(St√∂ter et al., 2019) is based on fully connected layers and a bi-LSTM. It uses multi-channel
Wiener filtering (Nugraha et al., 2016) to reduce artifacts. While the original Open-Unmix
is trained independently on each source, a multi-target version exists (Sawata et al., 2020),
through a shared averaged representation layer. D3Net (Takahashi and Mitsufuji, 2020) is
another architecture, based on dilated convolutions connected with dense skip connections. It
was before the competition the best performing spectrogram architecture, with an average SDR
of 6.0 dB on MusDB. Unlike previous methods which are based on masking, LaSAFT (Choi
et al., 2021) uses Complex-As-Channels (Choi et al., 2020) along with a U-Net (Ronneberger
et al., 2015) architecture. It is also single-target, however its weights are shared across targets,
using a weight modulation mechanism to select a specific source.
Waveform domain source separation was first explored by Llu√≠s et al. (2018), as well as Jansson
et al. (2017) and Stoller et al. (2018) with Wave-U-Net. However, those methods were lagging
in term of quality, almost 2 dB behind their spectrogram based competitors. Demucs (D√©fossez
et al., 2019) was built upon Wave-U-Net, using faster strided convolutions rather than explicit
downsampling, allowing for a much larger number of channels, but potentially introducing
aliasing artifacts as noted by Pons et al. (2021), and extra Gated Linear Unit layers (Dauphin
et al., 2017) and biLSTM. For the first time, waveform domain methods surpassed spectrogram
ones when considering the overall SDR (6.3 dB on MusDB), although its performance is still
inferior on the other and vocals sources. Conv-Tasnet (Luo and Mesgarani, 2019), a model
based on masking over a learnt time-frequency representation using dilated convolutions, was
also adapted to music source separation by D√©fossez et al. (2019), but suffered from more
artifacts and lower SDR.
To the best of our knowledge, no other work has studied true end-to-end hybrid source
separation, although other teams in the MDX competition used model blending from different
domains as a simpler post-training alternative.

Architecture
In this Section we present the structure of Hybrid Demucs, as well as the other additions that
were added to the original Demucs architecture.

Original Demucs
The original Demucs architecture (D√©fossez et al., 2019) is a U-Net (Ronneberger et al., 2015)
encoder/decoder structure. A BiLSTM (Hochreiter and Schmidhuber, 1997) is applied between

Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.

2

the encoder and decoder to provide long range context. The encoder and decoder have a
symetric structure. Each encoder layer is composed of a convolution with a kernel size of 8,
stride of 4 and doubling the number of channels (except for the first layer, which sets it to
a fix value, typically 48 or 64). It is followed by a ReLU, and a so called 1x1 convolution
with Gated Linear Unit activation (Dauphin et al., 2017), i.e. a convolution with a kernel
size of 1, where the first half of the channels modulates the second half through a sigmoid.
The 1x1 convolution double the channels, and the GLU halves them, keeping them constant
overall. Symetrically, a decoder layer sums the contribution from the U-Net skip connection
and the previous layer, apply a 1x1 convolution with GLU, then a transposed convolution that
halves the number of channels (except for the outermost layer), with a kernel size of 8 and
stride of 4, and a ReLU (except for the outermost layer). There are 6 encoder layers, and 6
decoder layers, for processing 44.1 kHz audio. In order to limit the impact of aliasing from the
outermost layers, the input audio is upsampled by a factor of 2 before entering the encoder,
and downsampled by a factor of 2 when leaving the decoder.

Hybrid Demucs
Overall architecture
Hybrid Demucs extends the original architecture with multi-domain analysis and prediction
capabilities. The model is composed of a temporal branch, a spectral branch, and shared layers.
The temporal branch takes the input waveform and process it like the standard Demucs. It
contains 5 layers, which are going to reduce the number of time steps by a factor of 45 = 1024.
Compared with the original architecture, all ReLU activations are replaced by Gaussian Error
Linear Units (GELU) (Hendrycks and Gimpel, 2016).
The spectral branch takes the spectrogram obtained from a STFT over 4096 time steps, with
a hop length of 1024. Notice that the number of time steps immediately matches that of the
output of the encoder of the temporal branch. In order to reduce the frequency dimension, we
apply the same convolutions as in the temporal branch, but along the frequency dimension.
Each layer reduces by a factor of 4 the number of frequencies, except for the 5th layer, which
reduces by a factor of 8. After being processed by the spectral encoder, the signal has only
one ‚Äúfrequency‚Äù left, and the same number of channels and sample rate as the output of the
temporal branch. The temporal and spectral representations are then summed before going
through a shared encoder/decoder layer which further reduces by 2 the number of time steps
(using a kernel size of 4). Its output serves both as the input of the temporal and spectral
decoder. Hybrid Demucs has a dual U-Net structure, with the temporal and spectral branches
having their respective skip connections.
The output of the spectral branch is inversed with the ISTFT, and summed with the temporal
branch output, giving the final model prediction. Due to this overall design, the model is free
to use whichever representation is most conveniant for different parts of the signal, even within
one source, and can freely share information between the two representations. The hybrid
architecture is represented on 1.
Padding for easy alignment
One difficulty was to properly align the spectral and temporal representations for any input
length. For an input length L, kernel size K, stride S and padding on each side P , the output
of a convolution is of length (L ‚àí K + 2 ‚àó P )/S + 1. Following the practice from models like
MelGAN (Kumar et al., 2019) we pad by P = (K ‚àí S)/2, giving an output of L/S, so that
matching the overall stride is now sufficiant to exactly match the length of the spectral and
temporal representations. We apply this padding both for the STFT, and convolution layers in
the temporal encoders.

Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.

3

+

ISTFT

T/1024 time steps, 2048 freq.

T time steps.

ZDecoder2 (Cin = 48, Cout = 4 ¬∑ 2 ¬∑ 2)

TDecoder1 (Cin = 48, Cout = 4 ¬∑ 2 ¬∑ 2)

T/1024 time steps, 512 freq.

T/4 time steps

ZDecoder2 (Cin = 96, Cout = 48)

TDecoder2 (Cin = 96, Cout = 48)

T/1024 time steps, 256 freq.

T/256 time steps

...

...

T/1024 time steps, 8 freq.

T/1024 time steps

ZDecoder5 (Cin = 768, Cout = 386)

TDecoder5 (Cin = 768, Cout = 386)
T/1024 time steps

Decoder6 (Cin = 1586, Cout = 768)
T/2048 time steps
Encoder6 (Cin = 768, Cout = 1586)
T/1024 time steps, 1 freq.

+

ZEncoder5 (Cin = 384, Cout = 768)

T/1024 time steps
TEncoder5 (Cin = 384, Cout = 768)
T/256 time steps

T/1024 time steps, 8 freq.
...

...
T/1024 time steps, 256 freq.
ZEncoder2 (Cin = 48, Cout = 96)
T/1024 time steps, 512 freq.
ZEncoder1 (Cin = 2 ¬∑ 2, Cout = 48)
T /1024 time steps 2048 freq.

T/16 time steps
TEncoder2 (Cin = 48, Cout = 96)
T/4 time steps
TEncoder1 (Cin = 2, Cout = 48)
T time steps

STFT

Figure 1: Hybrid Demucs architecture. The input waveform is processed both through a temporal
encoder, and first through the STFT followed by a spectral encoder. The two representations are
summed when their dimensions align. The decoder is built symmetrically. The output spectrogram go
through the ISTFT and is summed with the waveform outputs, giving the final model output. The Z
prefix is used for spectral layers, and T prefix for the temporal ones.

Frequency-wise convolutions
In the spectral branch, we use frequency-wise convolutions, dividing the number of frequency
bins by 4 with every layer. For simplicity we drop the highest bin, giving 2048 frequency
bins after the STFT. The input of the 5th layer has 8 frequency bins, which we reduce to 1
with a convolution with a kernel size of 8 and no padding. It has been noted that unlike the
time axis, the distribution of musical signals is not truely invariant to translation along the
frequency axis. Instruments have specific pitch range, vocals have well defined formants etc.
To account for that, Isik et al. (2020) suggest injecting an embedding of the frequency before
applying the convolution. We use the same approach, with the addition that we smooth the
initial embedding so that close frequencies have similar embeddings. We inject this embedding
just before the second encoder layer. We also investigated using specific weights for different
frequency bands. This however turned out more complex for a similar result.

Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.

4

Spectrogram representation
We investigated both with representing the spectrogram as complex numbers (Choi et al., 2020)
or as amplitude spectrograms. We also experimented with using Wiener filtering (Nugraha
et al., 2016), using Open-Unmix differentiable implementation (St√∂ter et al., 2019), which uses
an iterative procedure. Using more iterations at evaluation time is usually optimal, but sadly
doesn‚Äôt work well with the hybrid approach, as changing the spectrogram output, without the
waveform output being able to adapt will drastically reduce the SDR, and using a high number
of iterations at train time is prohibitively slow. In all cases, we differentiably transform the
spectrogram branch output to a waveform, summed to the waveform branch output, and the
final loss is applied in the waveform domain.

Compressed residual branches
The original Demucs encoder layer is composed of a convolution with kernel size of 8 and stride
of 4, followed by a ReLU, and of a convolution with kernel size of 1 followed by a GLU. Between
those two convolutions, we introduce two compressed residual branches, composed of dilated
convolutions, and for the innermost layers, a biLSTM with limited span and local attention.
Remember that after the first convolution of the 5th layer, the temporal and spectral branches
have the same shape. The 5th layer of each branch actually only contains this convolution,
with the compressed residual branch and 1x1 convolution being shared.
Inside a residual branch, all convolutions are with respect to the time dimension, and different
frequency bins are processed separately. There are two compressed residual branch per encoder
layer. Both are composed of a convolution with a kernel size of 3, stride of 1, dilation of 1
for the first branch and 2 for the second, and 4 times less output dimensions than the input,
followed by layer normalization (Ba et al., 2016) and a GELU activation.
For the 5th and 6th encoder layers, long range context is processed through a local attention
layer (see definition hereafter) as well as a biLSTM with 2 layers, inserted with a skip connection,
and with a maximum span of 200 steps. In practice, the input is splitted into frames of 200
time steps, with a stride of 100 steps. Each frame is processed concurrently, and for any time
step, the output from the frame for which it is the furthest away from the edge is kept.
Finally, and for all layers, a final convolution with a kernel size of 1 outputs twice as many
channels as the input dimension of the residual branch, followed by a GLU. This output is then
summed with the original input, after having been scaled through a LayerScale layer (Touvron
et al., 2021), with an initial scale of 1e‚àí3. A complete representation of the compressed
residual branches is given on 2.
Local attention
Local attention builds on regular attention (Vaswani et al., 2017) but replaces positional
embedding by a controllable penalty term that penalizes attending to positions that are far
away. Formally, the attention weights from position i to position j is given by
wi,j = softmax(QTi Kj ‚àí

4
X

kŒ≤i,k |i ‚àí j|)

k=1

where Qi are the queries and Kj are the keys. The values Œ≤i,k are obtained as the output of
a linear layer, initialized so that they are initially very close to 0. Having multiple Œ≤i,k with
different weights k allows the network to efficiently reduce its receptive field without requiring
Œ≤i,k to take large values. In practice, we use a sigmoid activation to derive the values Œ≤i,k .
Interestingly, a similar idea has been developed in NLP (Press et al., 2021), although with a
fixed penalty rather than a dynamic and learnt one done here.

Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.

5

Encoderi+1

Decoderi

GLU(Conv1d(Cout , 2 ¬∑ Cout , K = 1, S = 1))
LayerScale(init = 1e‚àí3)
GLU(LN(Conv1d(Cout /4, 2 ¬∑ Cout , K = 1)))
LocalAttention(heads = 4)

)
if i ‚àà {5, 6}

BiLSTM(layers = 2, span = 200)
GELU(LN(Conv1d(Cout , Cout /4, K = 3, D = 2)))
LayerScale(init = 1e‚àí3)
GLU(LN(Conv1d(Cout /4, 2 ¬∑ Cout , K = 1)))
LocalAttention(heads = 4)

)
if i ‚àà {5, 6}

BiLSTM(layers = 2, span = 200)
GELU(LN(Conv1d(Cout , Cout /4, K = 3, D = 1)))
GELU(Conv1d(Cin , Cout , K = 8, S = 4))
Encoderi‚àí1 or input

Figure 2: Representation of the compressed residual branches that are added to each encoder layer.
For the 5th and 6th layer, a BiLSTM and a local attention layer are added.

Stabilizing training
We observed that Demucs training could be unstable, especially as we added more layers
and increased the training set size with 150 extra songs. Loading the model just before its
divergence point, we realized that the weights for the innermost encoder and decoder layers
would get very large eigen values.
A first solution is to use group normalization (with 4 groups) just after the non residual
convolutions for the layers 5 and 6 of the encoder and the decoder. Using normalization on all
layers deteriorates performance, but using it only on the innermost layers seems to stabilize
training without hurting performance. Interestingly, when the training is stable (in particular
when trained only on MusDB), using normalization was at best neutral with respect to the
separation score, but never improved it, and considerably slowed down convergence during the
first half of the epochs. When the training was unstable, using normalization would improve
the overall performance as it allows the model to train for a larger number of epochs.
A second solution we investigated was to use singular value regularization (Yoshida and Miyato,
2017). While previous work used the power method iterative procedure, we obtained better
and faster approximations of the largest singular value using a low rank SVD method (Halko
et al., 2011). This solution has the advantage of always improving generalization, even when
the training was already stable. Sadly, it was not sufficient on its own to remove entirely
instabilities, but only to reduce them. Another down side was the longer training time due to
the extra low rank SVD evaluation. In the end, in order to both achieve the best performance
and remove entirely training instabilities, the two solutions were combined.

Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.

6

Experimental Results
Important: The results presented in this section are results obtained as part of the MDX
challenge. We provide easier to reproduce results and detailed ablation that were conducted
after the challenge in the Section "Reproducibility and Ablation" hereafter.

Datasets
The 2021 MDX challenge (Mitsufuji et al., 2021) offered two tracks: Track A, where only
MusDB HQ (Rafii et al., 2019) could be used for training, and Track B, where any data could
be used. MusDB HQ, released under mixed licensing1 is composed of 150 tracks, including 86
for the train set, 14 for the valid, and 50 for the test set. For Track B, we additionally trained
using 150 tracks for an internal dataset, and repurpose the test set of MusDB as training
data, keeping only the original validation set for model selection. Models are evaluated either
through the MDX AI Crowd API2 , or on the MusDB HQ test set.
Realistic remix of tracks
We achieved further gains, by fine tuning the models on a specifically crafted dataset, and with
longer training samples (30 seconds instead of 10). This dataset was built by combining stems
from separate tracks, while respecting a number of conditions, in particular beat matching and
pitch compatibility. Note that training from scratch on this dataset led to worse performance,
likely because the model could rely too much on melodic structure, while random remixing
forces the model to separate without this information.
We use librosa (McFee et al., 2015) for both beat tracking and tempo estimation, as well as
chromagram estimation. Beat tracking is applied only on the drum source, while chromagram
estimation is applied on the bass line. We aggregate the chromagram over time to a single
chroma distribution and find the optimal pitch shift between two stems to maximize overlap
(as measured by the L1 distance). We assume that the optimal shift for the bass line is the
same for the vocals and accompaniments. Similarly, we align the tempo and first beat. In
order to limit artifacts, we only allow two stems to blend if they require less than 3 semi-tones
of shift and 15% of tempo change.

Metrics
The MDX challenge introduced a novel Signal-To-Distortion measure. Another SDR measure
existed, as introduced by Vincent et al. (2006). The advantage of the new definition is its
simplicty and fast evaluation. The new definition is simply defined as
P
2
n ks(n)k + 
SDR = 10 log10 P
,
(1)
2
n ks(n) ‚àí sÃÇ(n)k + 
where s is the ground truth source, sÃÇ the estimated source, and n the time index. In order to
reliably compare to previous work, we will refer to this new SDR definition as nSDR, and to
the old definition as SDR. Note that when using nSDR on the MDX test set, the metric is
defined as the average across all songs. The evaluation on the MusDB test set follows the
traditional median across the songs of the median over all 1 second segments of each song.

Models
The model submitted to the competitions were actually bags of 4 models. For Track A, we
had to mix hybrid and non hybrid Demucs models, as the hybrid ones were having worse
performance on the bass source. On Track B, we used only hybrid models, as the extra training
1 https://github.com/sigsep/website/blob/master/content/datasets/assets/tracklist.csv
2 https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021

Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.

7

data allowed them to perform better for all sources. Note that a mix of Hybrid models using
CaC or real masking were used, mostly because it was too costly to reevaluate all models for
the competition. For details on the exact architecture and hyper-parameter used, we refer the
reader to our Github repository facebookresearch/demucs.
For the baselines, we report the numbers from the top participants at the MDX competition
(Mitsufuji et al., 2021). We focus particularly on the KUIELAB-MDX-Net model, which came
in second. This model builds on (Choi et al., 2020) and combines a pure spectrogram model
with the prediction from the original Demucs (D√©fossez et al., 2019) model for the drums and
bass sources. When comparing models on MusDB, we also report the numbers for some of
the best performing methods outside of the MDX competition, namely D3Net (Takahashi and
Mitsufuji, 2020) and ResUNetDecouple+ (Kong et al., 2021), as well as the original Demucs
model (D√©fossez et al., 2019). Note that those models were evaluated on MusDB (not HQ)
which lacks the frequency content between 16 kHz and 22kHz. This can bias the metrics.

Results on MDX
We provide the results from the top participants at the MDX competition on Table 1 for the
track A (trained on MusDB HQ only) and on Table 2 for track B (any training data). We
also report for track A the metrics for the Demucs architecture improved with the residual
branches, but without the spectrogram branch. The hybrid approach especially improves the
nSDR on the Other and Vocals source. Despite this improvement, the Hybrid Demucs model
is still performing worse than the KUIELAB-MDX-Net on those two sources. On Track B,
we notice again that the Hybrid Demucs architecture is very strong on the Drums and Bass
source, while lagging behind on the Other and Vocals source.
Table 1: Results of Hybrid Demucs on the MDX test set, when trained only on MusDB (track A)
using the nSDR metric.

Method

All

Drums

Bass

Other

Vocals

Hybrid Demucs
KUIELAB-MDX-Net
Music_AI

7.33
7.24
6.88

8.04
7.17
7.37

8.12
7.23
7.27

5.19
5.63
5.09

7.97
8.90
7.79

Table 2: Results of Hybrid Demucs on the MDX test set, when trained with extra training (track B)
using the nSDR metric.

Method

All

Drums

Bass

Other

Vocals

Hybrid Demucs
KUIELAB-MDX-Net
AudioShake

8.11
7.37
8.33

8.85
7.55
8.66

8.86
7.50
8.34

5.98
5.53
6.51

8.76
8.89
9.79

Results on MusDB
We show on Table 3 the SDR metrics as measured on the MusDB dataset. Again, Hybrid
Demucs achieves the best performance for the Drums and Bass source, while improving quite
a lot over waveform only Demucs for the Other and Vocals, but not enough to surpasse
KUIELAB-MDX-Net, which is purely spectrogram based for those two sources. Interestingly,
the best performance on the Vocals source is also achieved by ResUNetDecouple+ (Kong
et al., 2021), which uses a novel complex modulation of the input spectrogram.

Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.

8

Human evaluations
We also performed Mean Opinion Score human evaluations. We re-use the same protocol as
in (D√©fossez et al., 2019): we asked human subjects to evaluate a number of samples based
on two criteria: the absence of artifacts, and the absence of bleeding (contamination). Both
are evaluated on a scale from 1 to 5, with 5 being the best grade. Each subject is tasked with
evaluating 25 samples of 12 seconds, drawn randomly from the 50 test set tracks of MusDB.
All subjects have a strong experience with music (amateur and professional musicians, sound
engineers etc). The results are given on Table 4 for the quality, and 5 for the bleeding. We
observe strong improvements over the original Demucs, although we observe some regression
on the bass source when considering quality. The model KUIELAb-MDX-Net that came in
second at the MDX competition performs the best on vocals. The Hybrid Demucs architecture
however reduces by a large amount bleeding across all sources.
Table 3: Comparison on the MusDB (HQ for Hybrid Demucs) test set, using the original SDR metric.
This includes methods that did not participate in the competition. ‚ÄúMode‚Äù indicates if the waveform
(W) or spectrogram (S) domain is used. Model with a ‚Äú*‚Äù were evaluated on MusDB HQ.

Method

Mode

All

Drums

Bass

Other

Vocals

Hybrid Demucs*
Demucs v2
KUIELAB-MDX-Net*
D3Net
ResUNetDecouple+

S+W
W
S+W
S
S

7.68
6.28
7.47
6.01
6.73

8.24
6.86
7.20
7.01
6.62

8.76
7.01
7.83
5.25
6.04

5.59
4.42
5.90
4.53
5.29

8.13
6.84
8.97
7.24
8.98

Table 4: Mean Opinion Score results when asking to rate the quality and absence of artifacts in the
generated samples, from 1 to 5 (5 being the best grade). Standard deviation is around 0.15.

Method

All

Drums

Bass

Other

Vocals

Ground Truth
Hybrid Demucs
KUIELAB-MDX-Net
Demucs v2

4.12
2.83
2.86
2.36

4.12
3.18
2.70
2.62

4.25
2.58
2.68
2.89

3.92
2.98
2.99
2.31

4.18
2.55
3.05
1.78

Table 5: Mean Opinion Score results when asking to rate the absence of bleeding between the sources,
from 1 to 5 (5 being the best grade). Standard deviation is around 0.15.

Method

All

Drums

Bass

Other

Vocals

Ground Truth
Hybrid Demucs
KUIELAB-MDX-Net
Demucs v2

4.40
3.04
2.44
2.37

4.51
2.95
2.23
2.24

4.52
3.25
2.19
2.96

4.13
3.08
2.64
1.99

4.43
2.88
2.66
2.46

Reproducibility and Ablation
In this section, we provide ablation of the performance of the model, as well as a simpler setup
for reproducing the performance of the model submitted to MDX Track A. Note that the
numbers and analysis presented here might differ slightly from the ones presented up to now,
and should be preferred when referring to this work.

Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.

9

Reproducibility
The model submitted to the MDX competition Track A used heterogeneous configurations, as
we used any model that was sufficiently trained at any given time. This led to a complex bag
of 4 models, some of which were used only for some sources. While suitable for a competition,
such a complex model does get in the way of reproducing easily the performance achieved.
The training grids for all the models presented in this section can be found on GitHub repository
facebookresearch/demucs, in the file demucs/grids/repro.py, and demucs/grids/repro_
ft.py for the fine tuned models.
We reproduced the performance of the model submitted to Track A by training two time
domain Demucs (with the residual branches depicted on Figure 2), and two hybrid Demucs
using Complex-As-Channels representation. All four models were trained for 600 epochs, using
the SVD penalty and exponential moving average on the weights. Within each domain, only
the random seed changes between the two models. The four models were fined tuned on the
realistic remix of tracks dataset. The predictions of the four models are averaged into the final
prediction, with equal weights over all sources.
As a first ablation, we also form one bag composed of the two time domain only models, and
another bag with the two hybrid models only. For fairness, we evaluate each model twice with
a random shift, which is known to improve the performance (D√©fossez et al., 2019). We also
compare to the original Demucs model, retrained on MusDB HQ, using 10 random shifts, as
done in (D√©fossez et al., 2019).
The results are presented on Table 6. We reach an overall SDR of 7.64 dB, just 0.04 dB of
the model submitted to MDX Track A. We notice a difference in performance between time
only, and hybrid only bags only for the bass source and the other source. We still decided
to use the same weights over all sources for each type of model for simplicity. We can see
the advantage of averaging multiple models, as the combination of both time only and hybrid
only model surpasses either ones individually for instance on the drums or the vocals sources.
Note however that when training with extra training data, e.g. for the MDX Track B models,
the hybrid models were always better than the time only ones.
Table 6: Comparison on the MusDB HQ test set, using the original SDR metric of different bags of
models, as well as with the original Demucs v2 model retrained on MusDB HQ. ‚ÄúMode‚Äù indicates if
the waveform (W) or spectrogram (S) domain is used.

Method

Mode

All

Drums

Bass

Other

Vocals

Bag time + hybrid
Bag time only
Bag hybrid only
Demucs v2 HQ

S+W
W
S+W
W

7.64
7.27
7.34
6.17

8.12
7.57
7.96
6.54

8.43
8.38
7.85
7.08

5.65
5.17
5.63
4.21

8.35
7.96
7.95
6.85

Ablation
We report on Table 7 a short ablation study of the model. We start from a time only improved
Demucs, e.g. trained with residual branches, local attention and svd penalty. We can first
oberve the effect of fine tuning on a set of realistic remixes, which improves by 0.3 dB the
SDR overall. Further gains are achieved using the bagging. Using Exponential Moving Average
on the weights improves the SDR by 0.2dB. The effect of the SVD penalty is more contrasted,
with on overall gain of 0.1dB, mainly due to the improved vocals (+0.7 dB), but with a
deterioration on the drums source (-0.4 dB).
Finally, removing the LSTM or the local attention in the residual branches lead to a strong
decrease of the SDR. Interestingly, the local attention is the most important, despite the
absence of positional embedding. One decision taken during the challenge was to switch to

Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.

10

GELU instead of ReLU. The ablation indicates that no real gain is achieved here.
Table 7: Ablation study, all models are trained and evaluated on MusDB HQ. The base model is a
time only improved Demucs, with local attention, residual branches and svd penalty. Note that we
report single model performance instead of bags of model.

Model

All

Drums

Bass

Other

Vocals

Improved Time Demucs
+ fine tuning
+ fine tuning and bagging
- LSTM in branch
- Local Attention
- SVD penalty
- EMA on weights
- GELU + ReLU

6.83
7.11
7.27
6.44
6.29
6.73
6.63
6.84

7.06
7.42
7.57
6.66
6.39
7.45
6.99
7.19

7.78
8.18
8.38
6.68
6.76
8.01
7.36
7.81

4.81
5.08
5.17
4.89
4.68
4.48
4.74
4.73

7.65
7.75
7.96
7.54
7.33
6.98
7.43
7.63

Conclusion
We introduced a number of architectural changes to the Demucs architecture that greatly
improved the quality of source separation for music. On the MusDB HQ benchark, the gain is
around 1.4 dB. Those changes include compressed residual branches with local attention and
chunked biLSTM, and most importantly, a novel hybrid spectrogram/temporal domain U-Net
structure, with parallel temporal and spectrogram branches, that merge into a common core.
Those changes allowed to achieve the first rank at the 2021 Sony Music DemiXing challenge,
and translated into strong improvements of the overall quality and absence of bleeding between
sources as measured by human evaluations. For all its gain, one limitation of our approach is
the increased complexity of the U-Net encoder/decoder, requiring careful alignmement of the
temporal and spectral signals through well shaped convolutions.

Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.

11

References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Woosung Choi, Minseok Kim, Jaehwa Chung, Daewon Lee, and Soonyoung Jung. Investigating u-nets
with various intermediate blocks for spectrogram-based singing voice separation. In ISMIR, editor,
21th International Society for Music Information Retrieval Conference, 2020.
Woosung Choi, Minseok Kim, Jaehwa Chung, and Soonyoung Jung. Lasaft: Latent source attentive
frequency transformation for conditioned source separation. In IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2021.
Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutional networks. In Proceedings of the International Conference on Machine Learning, 2017.
Alexandre D√©fossez, Nicolas Usunier, L√©on Bottou, and Francis Bach. Music source separation in the
waveform domain. arXiv preprint arXiv:1911.13254, 2019.
Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53(2):
217‚Äì288, 2011.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,
2016.
Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735‚Äì1780, 1997.
Umut Isik, Ritwik Giri, Neerad Phansalkar, Jean-Marc Valin, Karim Helwani, and Arvindh Krishnaswamy.
Poconet: Better speech enhancement with frequency-positional embeddings, semi-supervised
conversational data, and biased loss. arXiv preprint arXiv:2008.04470, 2020.
Andreas Jansson, Eric Humphrey, Nicola Montecchio, Rachel Bittner, Aparna Kumar, and Tillman
Weyde. Singing voice separation with deep u-net convolutional networks. In ISMIR, 2017.
Qiuqiang Kong, Yin Cao, Haohe Liu, Keunwoo Choi, and Yuxuan Wang. Decoupling magnitude and
phase estimation with deep resunet for music source separation. In 22th International Society for
Music Information Retrieval Conference, 2021.
Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo,
Alexandre de Br√©bisson, Yoshua Bengio, and Aaron C Courville. Melgan: Generative adversarial
networks for conditional waveform synthesis. In Advances in Neural Information Processing Systems,
2019.
Francesc Llu√≠s, Jordi Pons, and Xavier Serra. End-to-end music source separation: is it possible in the
waveform domain? arXiv preprint arXiv:1810.12, 2018.
Yi Luo and Nima Mesgarani. Conv-tasnet: Surpassing ideal time‚Äìfrequency magnitude masking for
speech separation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2019.
Yi Luo, Zhuo Chen, and Takuya Yoshioka. Dual-path rnn: efficient long sequence modeling for
time-domain single-channel speech separation. In IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), 2020.
Brian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric Battenberg, and Oriol
Nieto. librosa: Audio and music signal analysis in python. In Proceedings of the 14th python in
science conference, 2015.
Yuki Mitsufuji, Giorgio Fabbro, Stefan Uhlich, and Fabian-Robert St√∂ter. Music demixing challenge
2021. arXiv preprint arXiv:2108.13559, 2021.
Aditya Arie Nugraha, Antoine Liutkus, and Emmanuel Vincent. Multichannel music separation with
deep neural networks. In Signal Processing Conference (EUSIPCO). IEEE, 2016.

Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.

12

Jordi Pons, Santiago Pascual, Giulio Cengarle, and Joan Serr√†. Upsampling artifacts in neural audio
synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
2021.
Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables
input length extrapolation. 2021.
Zafar Rafii, Antoine Liutkus, Fabian-Robert St√∂ter, Stylianos Ioannis Mimilakis, and Rachel Bittner.
The musdb18 corpus for music separation, 2017.
Zafar Rafii, Antoine Liutkus, Fabian-Robert St√∂ter, Stylianos Ioannis Mimilakis, and Rachel Bittner.
MUSDB18-HQ - an uncompressed version of musdb18, December 2019. URL https://doi.org/10.
5281/zenodo.3338373.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical image computing and computer-assisted
intervention, 2015.
Ryosuke Sawata, Stefan Uhlich, Shusuke Takahashi, and Yuki Mitsufuji. All for one and one for all:
Improving music separation by bridging networks. 2020.
Daniel Stoller, Sebastian Ewert, and Simon Dixon. Wave-u-net: A multi-scale neural network for
end-to-end audio source separation. arXiv preprint arXiv:1806.03185, 2018.
F.-R. St√∂ter, S. Uhlich, A. Liutkus, and Y. Mitsufuji. Open-unmix - a reference implementation for
music source separation. Journal of Open Source Software, 2019. doi: 10.21105/joss.01667.
Fabian-Robert St√∂ter, Antoine Liutkus, and Nobutaka Ito. The 2018 signal separation evaluation
campaign. In 14th International Conference on Latent Variable Analysis and Signal Separation,
2018.
Naoya Takahashi and Yuki Mitsufuji. D3net: Densely connected multidilated densenet for music source
separation. arXiv preprint arXiv:2010.01733, 2020.
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv√© J√©gou. Going
deeper with image transformers. arXiv preprint arXiv:2103.17239, 2021.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing
systems, 2017.
Emmanuel Vincent, R√©mi Gribonval, and C√©dric F√©votte. Performance measurement in blind audio
source separation. IEEE Transactions on Audio, Speech and Language Processing, 2006.
Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of
deep learning. arXiv preprint arXiv:1705.10941, 2017.
Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR, 2016.

Hybrid Spectrogram and Waveform Source Separation. Proceedings of the MDX Workshop, 2021.

13
