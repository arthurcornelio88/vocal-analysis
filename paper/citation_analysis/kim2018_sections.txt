4. DISCUSSIONS AND CONCLUSION
In this paper, we presented a novel data-driven method for monophonic pitch tracking based on a deep convolutional neural network
operating on time-domain input, CREPE. We showed that CREPE
obtains state-of-the-art results, outperforming pYIN and SWIPE on
two datasets with homogeneous and heterogeneous timbre respectively. Furthermore, we showed that CREPE remains highly accurate
even at a very strict evaluation threshold of just 10 cents. We also
showed that in most cases CREPE is more robust to added noise.
Ideally, we want the model to be invariant to all transformations
that do not affect pitch, such as changes due to distortion and reverberation. Some invariance can be induced by the architectural design
of the model, such as the translation invariance induced by pooling
layers in our model as well as in deep image classification models.
However, it is not as straightforward to design the model architecture
to specifically ignore other pitch-preserving transformations. While
it is still an intriguing problem to build an architecture to achieve
this, we could use data augmentation to generate transformed and
degraded inputs that can effectively make the model learn the invariance. The robustness of the model could also be improved by applying pitch-shifts as data augmentation [29] to cover a wider pitch
range for every instrument. In addition to data augmentation, various
sources of audio timbre can be obtained from software instruments;
NSynth [30] is an example where the training dataset is generated
from the sound of software instruments.
Pitch values tend to be continuous over time, but CREPE estimates the pitch of every frame independently without using any temporal tracking, unlike pYIN which exploits this by using an HMM to
enforce temporal smoothness. We can potentially improve the performance of CREPE even further by enforcing temporal smoothness.
In the future, we plan to do this by means of adding recurrent architecture to our model, which could be trained jointly with the convolutional front-end in the form of a convolutional-recurrent neural
network (CRNN).

5. REFERENCES
[1] Rachel M Bittner, Justin Salamon, Mike Tierney, Matthias
Mauch, Chris Cannam, and Juan Pablo Bello, “Medleydb: A
multitrack dataset for annotation-intensive mir research.,” in
Proceedings of the 15th ISMIR Conference, 2014, vol. 14, pp.
================================================================================
