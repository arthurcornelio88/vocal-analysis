# TODO - Conformidade com Metodologia do Artigo

**Data**: 2025-01-21
**Status**: AnÃ¡lise completa da implementaÃ§Ã£o vs metodologia descrita no artigo

---

## âœ… O QUE JÃ ESTÃ IMPLEMENTADO

- [x] **CREPE para f0**: ExtraÃ§Ã£o via torchcrepe com modelo CNN (extraction.py:54)
- [x] **Parselmouth/Praat para HNR**: Harmonicity temporal (extraction.py:74)
- [x] **CPPS**: Cepstral Peak Prominence Smoothed via Praat (extraction.py:80-83)
- [x] **Mono 44.1kHz**: ConversÃ£o automÃ¡tica no load (audio.py:24)
- [x] **XGBoost**: Estrutura de classificador pronta (classifier.py)
- [x] **Clustering GMM**: ClassificaÃ§Ã£o nÃ£o-supervisionada M1/M2 (exploratory.py:95)
- [x] **Plots acadÃªmicos**: VisualizaÃ§Ãµes com seaborn (plots.py, exploratory.py)
- [x] **RelatÃ³rio LLM**: GeraÃ§Ã£o narrativa com Gemini multimodal (llm_report.py)

---

## âš ï¸ DIVERGÃŠNCIAS CRÃTICAS (Implementado diferente da metodologia)

### 1. NormalizaÃ§Ã£o de Ãudio
- **Metodologia**: `-3 dBFS de pico`
- **Implementado**: FunÃ§Ã£o existe em `audio.py:28` mas **NÃƒO Ã‰ CHAMADA**
- **Impacto**: Afeta energia espectral e comparaÃ§Ãµes entre mÃºsicas
- **CorreÃ§Ã£o**: Adicionar `normalize_audio()` no pipeline de `load_audio()`

### 2. Hop Length
- **Metodologia**: `10 ms` (441 samples @ 44.1kHz)
- **Implementado**: `20 ms` (882 samples @ 44.1kHz) - `extraction.py:26`
- **Impacto**: ResoluÃ§Ã£o temporal 2x menor (menos frames, menos precisÃ£o em ornamentos rÃ¡pidos)
- **CorreÃ§Ã£o**: Mudar `hop_length: int = 882` para `hop_length: int = 441`

### 3. Threshold de ConfianÃ§a (f0)
- **Metodologia**: `> 0.8`
- **Implementado**: `> 0.5` - `exploratory.py:45` e `process_ademilde.py:47`
- **Impacto**: Pode incluir detecÃ§Ãµes de pitch menos confiÃ¡veis
- **CorreÃ§Ã£o**: Mudar `df["confidence"] > 0.5` para `df["confidence"] > 0.8` em todos os arquivos

### 4. Janelamento Hanning 25ms
- **Metodologia**: ExplÃ­cito "janelamento de Hanning com frames de 25 ms"
- **Implementado**: CREPE faz internamente (nÃ£o configurÃ¡vel pelo usuÃ¡rio)
- **Impacto**: Baixo (CREPE usa janelamento prÃ³prio otimizado)
- **CorreÃ§Ã£o**: Documentar no cÃ³digo que CREPE usa seu prÃ³prio janelamento

---

## âŒ FEATURES NÃƒO IMPLEMENTADAS (Citadas na metodologia)

### 1. Jitter (ppq5)
- **DescriÃ§Ã£o**: Period Perturbation Quotient (5 perÃ­odos) - mede instabilidade da vibraÃ§Ã£o das pregas
- **Uso**: Quantificar estabilidade glÃ³tica em M1 vs M2
- **ImplementaÃ§Ã£o necessÃ¡ria**:
```python
# Em extraction.py, adicionar Ã  funÃ§Ã£o extract_bioacoustic_features()
point_process = parselmouth.praat.call(sound, "To PointProcess (periodic, cc)", fmin, fmax)
jitter_ppq5 = parselmouth.praat.call(point_process, "Get jitter (ppq5)", 0, 0, 0.0001, 0.02, 1.3)
```
- **Arquivo**: `src/vocal_analysis/features/extraction.py`

### 2. Shimmer (apq11)
- **DescriÃ§Ã£o**: Amplitude Perturbation Quotient (11 perÃ­odos) - mede variaÃ§Ã£o de amplitude
- **Uso**: Quantificar regularidade de amplitude em M1 vs M2
- **ImplementaÃ§Ã£o necessÃ¡ria**:
```python
shimmer_apq11 = parselmouth.praat.call(point_process, "Get shimmer (apq11)", 0, 0, 0.0001, 0.02, 1.3, 1.6)
```
- **Arquivo**: `src/vocal_analysis/features/extraction.py`

### 3. Formantes F1-F4 via LPC
- **DescriÃ§Ã£o**: Primeiros 4 formantes via Linear Predictive Coding (mÃ©todo de Burg)
- **Uso**: Detectar aproximaÃ§Ã£o de formantes na "zona de fala"
- **ImplementaÃ§Ã£o necessÃ¡ria**:
```python
# Via Praat/Parselmouth
formants = sound.to_formant_burg(time_step=time_step, max_number_of_formants=5, maximum_formant=5500)
f1_values = formants.to_array(formant_number=1)  # Array temporal
f2_values = formants.to_array(formant_number=2)
f3_values = formants.to_array(formant_number=3)
f4_values = formants.to_array(formant_number=4)
```
- **Arquivo**: `src/vocal_analysis/features/extraction.py`
- **Nota**: Arrays temporais precisarÃ£o ser sincronizados com f0/HNR

### 4. Energia Espectral (Energy)
- **DescriÃ§Ã£o**: Energia RMS por frame
- **Uso**: Feature usada no XGBoost (`classifier.py:29` espera coluna 'energy')
- **ImplementaÃ§Ã£o necessÃ¡ria**:
```python
# Via librosa ou cÃ¡lculo manual
import librosa
energy = librosa.feature.rms(y=audio, frame_length=int(0.025*sr), hop_length=hop_length)[0]
```
- **Arquivo**: `src/vocal_analysis/features/extraction.py`
- **CRÃTICO**: Classifier jÃ¡ espera essa feature!

### 5. VAD (Voice Activity Detection) - webrtcvad
- **Metodologia**: "algoritmo de detecÃ§Ã£o de atividade de voz (VAD) baseado em energia (biblioteca webrtcvad)"
- **Implementado**: Usa threshold de `confidence > 0.5` do CREPE
- **Impacto**: Moderado (CREPE jÃ¡ filtra silÃªncios razoavelmente bem)
- **ImplementaÃ§Ã£o necessÃ¡ria**:
```python
# PrÃ©-processamento adicional (opcional)
import webrtcvad
vad = webrtcvad.Vad(mode=3)  # Modo 3 = mais agressivo
# Processar em frames de 10/20/30ms e filtrar silÃªncios
```
- **Arquivo**: Novo mÃ³dulo `src/vocal_analysis/preprocessing/vad.py`
- **Prioridade**: BAIXA (funcionalidade jÃ¡ existe via confidence)

---

## ğŸ”§ MELHORIAS PARA AGILIDADE ARTICULATÃ“RIA

**Problema**: CPPS nÃ£o mede agilidade articulatÃ³ria (canto rÃ¡pido).

**Features necessÃ¡rias** (nÃ£o citadas na metodologia mas Ãºteis para a anÃ¡lise):

### 1. Taxa de MudanÃ§a de Pitch (f0 velocity)
```python
f0_velocity = np.diff(f0) / np.diff(time)  # Hz/s
f0_acceleration = np.diff(f0_velocity) / np.diff(time[1:])
```

### 2. DetecÃ§Ã£o de Notas e DuraÃ§Ãµes
```python
# Segmentar f0 em notas estÃ¡veis vs transiÃ§Ãµes
note_onsets = detect_onsets(f0, threshold_change=20)  # 20 Hz de mudanÃ§a
note_durations = np.diff(note_onsets) * time_step
mean_note_duration = np.mean(note_durations)
```

### 3. Taxa SilÃ¡bica (Syllable Rate)
```python
# Proxy: contar picos de energia
from scipy.signal import find_peaks
syllable_peaks = find_peaks(energy, distance=int(0.1/time_step))[0]
syllable_rate = len(syllable_peaks) / total_duration  # sÃ­labas/segundo
```

**Arquivo**: Novo mÃ³dulo `src/vocal_analysis/features/articulation.py`
**Prioridade**: MÃ‰DIA (ajudaria a justificar aspectos do Choro)

---

## ğŸ“Š INTEGRAÃ‡ÃƒO XGBoost (4.2.4 - ClassificaÃ§Ã£o M1/M2)

**Status atual**: CÃ³digo existe mas nÃ£o estÃ¡ integrado ao pipeline.

### O que falta:
1. âœ… Modelo XGBoost implementado (`classifier.py`)
2. âŒ ExtraÃ§Ã£o de `energy` (feature faltando)
3. âŒ Labels de treinamento (rÃ³tulos M1/M2)
4. âŒ IntegraÃ§Ã£o no pipeline de anÃ¡lise

### OpÃ§Ãµes de implementaÃ§Ã£o:

**OpÃ§Ã£o A - Clustering como Pseudo-Labels** (jÃ¡ funciona parcialmente):
```python
# exploratory.py jÃ¡ faz isso com GMM
# Substituir GMM por XGBoost treinado em labels do GMM
labels_gmm = gmm.fit_predict(features)
model_xgb = xgb.XGBClassifier()
model_xgb.fit(features, labels_gmm)
```

**OpÃ§Ã£o B - Rotulagem Manual** (ideal mas trabalhoso):
- Ouvir trechos e rotular M1/M2 manualmente
- Treinar XGBoost supervisionado

**OpÃ§Ã£o C - Threshold HÃ­brido** (mais simples):
```python
# Usar f0 + HNR + CPPS + energy
# M1: f0 < 400 Hz AND (HNR > threshold OR CPPS > threshold)
# M2: f0 >= 400 Hz OR (HNR low AND CPPS low)
```

---

## ğŸ¯ PRIORIDADES DE IMPLEMENTAÃ‡ÃƒO

### ALTA PRIORIDADE (DivergÃªncias crÃ­ticas)
1. [ ] Corrigir hop_length para 441 samples (10ms)
2. [ ] Integrar normalizaÃ§Ã£o -3dBFS no pipeline
3. [ ] Mudar threshold confianÃ§a para 0.8
4. [ ] **Implementar extraÃ§Ã£o de Energia** (classifier espera!)

### MÃ‰DIA PRIORIDADE (Features citadas na metodologia)
5. [ ] Implementar Jitter (ppq5)
6. [ ] Implementar Shimmer (apq11)
7. [ ] Implementar Formantes F1-F4
8. [ ] Integrar XGBoost no pipeline de classificaÃ§Ã£o

### BAIXA PRIORIDADE (Melhorias/Opcional)
9. [ ] Implementar webrtcvad (confidence CREPE jÃ¡ funciona)
10. [ ] Features de agilidade articulatÃ³ria (f0 velocity, taxa silÃ¡bica)
11. [ ] Documentar janelamento CREPE

---

## ğŸ“ ARQUIVOS A MODIFICAR

```
src/vocal_analysis/
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ extraction.py         # Adicionar: Jitter, Shimmer, Formantes, Energy
â”‚   â””â”€â”€ articulation.py       # NOVO: f0 velocity, taxa silÃ¡bica
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ audio.py              # Integrar normalize_audio no load_audio
â”‚   â””â”€â”€ vad.py                # NOVO (opcional): webrtcvad
â”œâ”€â”€ modeling/
â”‚   â””â”€â”€ classifier.py         # Integrar no pipeline (run_analysis.py)
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ exploratory.py        # Atualizar threshold confianÃ§a
â”‚   â””â”€â”€ run_analysis.py       # Adicionar etapa XGBoost
â””â”€â”€ preprocessing/
    â””â”€â”€ process_ademilde.py   # Atualizar threshold confianÃ§a, hop_length
```

---

## ğŸ§ª TESTES NECESSÃRIOS

ApÃ³s implementar as correÃ§Ãµes:

```bash
# 1. Reprocessar Ã¡udios com novos parÃ¢metros
uv run python -m vocal_analysis.preprocessing.process_ademilde

# 2. Verificar que todas features foram extraÃ­das
# CSV deve ter colunas: time, f0, confidence, hnr, cpps_global, jitter, shimmer, f1, f2, f3, f4, energy

# 3. Rodar anÃ¡lise completa
uv run python -m vocal_analysis.analysis.run_analysis

# 4. Validar classificaÃ§Ã£o M1/M2
# Verificar se separaÃ§Ã£o faz sentido perceptualmente
```

---

## ğŸ“š REFERÃŠNCIAS PENDENTES

- [ ] Henrich et al., 2014 - Para validar premissa CPPS em M1 vs M2
- [ ] Kim et al., 2018 - ReferÃªncia do CREPE (jÃ¡ citado corretamente)
- [ ] Boersma & Weenink, 2023 - Praat (jÃ¡ citado corretamente)

---

## â­ï¸ PRÃ“XIMOS PASSOS (para amanhÃ£)

1. **Corrigir divergÃªncias crÃ­ticas** (hop_length, normalizaÃ§Ã£o, threshold)
2. **Implementar Energy** (urgente - classifier precisa)
3. **Implementar Jitter e Shimmer** (metodologia exige)
4. **Testar pipeline completo** com dados reais
5. **Decidir sobre Formantes** (sÃ£o necessÃ¡rios?)
6. **Integrar XGBoost** ou manter GMM?

---

**Nota**: O cÃ³digo atual funciona e gera resultados vÃ¡lidos, mas diverge da metodologia escrita. Essas correÃ§Ãµes garantirÃ£o conformidade acadÃªmica com o artigo.
